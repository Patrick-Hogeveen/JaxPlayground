{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, key):\n",
    "        wkey, bkey = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(wkey, (out_size, in_size))\n",
    "        self.bias = jax.random.normal(bkey, (out_size,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.weight @ x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(weight=f32[3,2], bias=f32[3])\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "@jax.grad\n",
    "def loss_fn(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)\n",
    "\n",
    "batch_size, in_size, out_size = 32, 2, 3\n",
    "model = Linear(in_size, out_size, key=jax.random.PRNGKey(0))\n",
    "x = jax.numpy.zeros((batch_size, in_size))\n",
    "y = jax.numpy.zeros((batch_size, out_size))\n",
    "grads = loss_fn(model, x, y)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "    extra_bias: jax.Array\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        # These contain trainable parameters.\n",
    "        self.layers = [eqx.nn.Linear(2, 8, key=key1),\n",
    "                       eqx.nn.Linear(8, 8, key=key2),\n",
    "                       eqx.nn.Linear(8, 2, key=key3)]\n",
    "        # This is also a trainable parameter.\n",
    "        self.extra_bias = jax.numpy.ones(2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = jax.nn.relu(layer(x))\n",
    "        return self.layers[-1](x) + self.extra_bias\n",
    "\n",
    "@jax.jit  # compile this function to make it run fast.\n",
    "@jax.grad  # differentiate all floating-point arrays in `model`.\n",
    "def loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)  # vectorise the model over a batch of data\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)  # L2 loss\n",
    "\n",
    "x_key, y_key, model_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "# Example data\n",
    "x = jax.random.normal(x_key, (100, 2))\n",
    "y = jax.random.normal(y_key, (100, 2))\n",
    "model = NeuralNetwork(model_key)\n",
    "# Compute gradients\n",
    "grads = loss(model, x, y)\n",
    "# Perform gradient descent\n",
    "learning_rate = 0.1\n",
    "new_model = jax.tree_util.tree_map(lambda m, g: m - learning_rate * g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        self.layers = [eqx.nn.Linear(2, 8, key=key1),\n",
    "                       jax.nn.relu,\n",
    "                       eqx.nn.Linear(8, 2, key=key2)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "x_key, y_key, model_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "x, y = jax.random.normal(x_key, (100, 2)), jax.random.normal(y_key, (100, 2))\n",
    "model = NeuralNetwork2(model_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork2(\n",
       "  layers=[\n",
       "    Linear(\n",
       "      weight=f32[8,2],\n",
       "      bias=f32[8],\n",
       "      in_features=2,\n",
       "      out_features=8,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    None,\n",
       "    Linear(\n",
       "      weight=f32[2,8],\n",
       "      bias=f32[2],\n",
       "      in_features=8,\n",
       "      out_features=2,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ft.partial(jax.jit, static_argnums=1)  # `static` must be a PyTree of non-arrays.\n",
    "@jax.grad  # differentiates with respect to `params`, as it is the first argument\n",
    "def loss(params, static, x, y):\n",
    "    model = eqx.combine(params, static)\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)\n",
    "\n",
    "params, static = eqx.partition(model, eqx.is_array)\n",
    "loss(params, static, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork2(\n",
       "  layers=[\n",
       "    Linear(\n",
       "      weight=f32[8,2],\n",
       "      bias=f32[8],\n",
       "      in_features=2,\n",
       "      out_features=8,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    None,\n",
       "    Linear(\n",
       "      weight=f32[2,8],\n",
       "      bias=f32[2],\n",
       "      in_features=8,\n",
       "      out_features=2,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@eqx.filter_jit\n",
    "@eqx.filter_grad\n",
    "def loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)\n",
    "\n",
    "loss(model, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
